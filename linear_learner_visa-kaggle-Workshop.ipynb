{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Amazon SageMaker builtin algorithm to predict fraud "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preprocessing data\n",
    "If you are intersted in learning about preprocessing data, you should start here, otherwise you could simply start from part 2, when we load the data from npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your user number to the name variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"sagemaker-user-\" # For example: sagemaker-user-1\n",
    "if name.endswith('-'):\n",
    "    raise Exception('you must add your user number. For example: sagemaker-user-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run me\n",
    "# get the data from s3\n",
    "region = boto3.session.Session().region_name\n",
    "bucket = 'sagemaker-eu-west-1-483308273948'\n",
    "original_key = 'visa-kaggle/original.csv'\n",
    "protocol=\"s3://\"\n",
    "datafile = 'data/original.csv'\n",
    "prefix = name+ '/dataset'\n",
    " \n",
    "# get sagemaker IAM role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# start sagemaker session\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, spend a minute getting familiar with the following tools:  \n",
    "1. [Pandans](https://bit.ly/2y1DVeI) - A data manipulation and analysis library.\n",
    "2. [NumPy](https://en.wikipedia.org/wiki/NumPy) - A multi-dimensional arrays and matrices library for Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data ingestion, exploration and preperation\n",
    "We're going to do the following main steps:\n",
    "1. Downloding file locally to the Nootbook instance\n",
    "2. Loading file into [Pandas](https://bit.ly/2y1DVeI) for inspection  \n",
    "3. Converting data to [numpy](https://en.wikipedia.org/wiki/NumPy)\n",
    "4. Shuffling the data - Think why it's required...\n",
    "5. Breaking up each data set to data and label - Label is what we're trying to predict - think why it's required\n",
    "6. Spliting data into training and test datasets - [Read why here](https://en.wikipedia.org/wiki/Training,_test,_and_validation_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Downloading the file to a local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "with open(datafile, 'wb') as f:\n",
    "    boto3.client('s3').download_fileobj(bucket, original_key, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading data into pandas for inspection\n",
    "Read the local CSV and understand it's format.\n",
    "Use pandas to load the file\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(datafile)\n",
    "```\n",
    "\n",
    "and read the first 5 lines using:\n",
    "\n",
    "```python\n",
    "df.head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loading data into pandas for inspection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above has 31 columns.  \n",
    "The 'Class' colume tells whether the transaction was a fraud or not. This is what we'd like to predict.  \n",
    "Each transaction has 30 fields that the algorithm would learn: Time, Amount, and V1-V28 columes that had their colume names and values anonymized (the algorithm won't care). \n",
    "  \n",
    "Using Pandas, we can get more info about the data.\n",
    "```python \n",
    "print(df['Class'].value_counts())\n",
    "``` \n",
    "Tis will shows us that there are 284,807 records in this dataset, but only 492 of them are fraud.\n",
    "\n",
    "Let's visualize by:\n",
    "```python \n",
    "df['Class'].value_counts().plot(kind='pie')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the command above to examine the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Converting pandas to numpy\n",
    "We'll convert the data to numpy for data shuffling, data manipulation and extracting the labels from the dataset.\n",
    "\n",
    "Run \n",
    "```python\n",
    "raw_data = df.values\n",
    "``` \n",
    "to convert the df to numpy.  \n",
    "```python \n",
    "raw_data.shape\n",
    "``` \n",
    "will print the structure of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Data Into Numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4+5 Shuffling the data and spliting between data and labels\n",
    "\n",
    "By shuffeling the data we make sure that the order of the data will not affect the learning process.  \n",
    "\n",
    "using \n",
    "```python\n",
    "np.random.seed(123)\n",
    "``` \n",
    "we can configure the numpy random generator to use a constant seed so results are reproducible.\n",
    "  \n",
    "shuffle the data \n",
    "```\n",
    "np.random.shuffle(raw_data)\n",
    "```\n",
    "\n",
    "and split the data between the 30 explaining columes, and the colume the colume we'll predict 'Class'.\n",
    "\n",
    "```\n",
    "label = raw_data[:, -1]\n",
    "data = raw_data[:, :-1]\n",
    "```\n",
    "\n",
    "let's make sure that both have the same number of records:\n",
    "```python\n",
    "print(\"label_shape = {}; data_shape= {}\".format(label.shape, data.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the data and splitting between data and label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Spliting data into training and test datasets\n",
    "In this example we'll use 60% of the data for training and 40% for test.\n",
    "Training data is used to train the model. Once trained, test data is used to evaluate the models' accuracy. test data is never used during traning.\n",
    "\n",
    "we can get the training dataset size using:\n",
    "```python\n",
    "train_size = int(data.shape[0]*0.6)\n",
    "```\n",
    "\n",
    "we'll split both the training and test data sets (data and labels)\n",
    "```python\n",
    "train_data  = data[:train_size, :]\n",
    "test_data = data[train_size:, :]\n",
    "\n",
    "train_label = label[:train_size]\n",
    "test_label = label[train_size:]\n",
    "```\n",
    "\n",
    "We'll verifiy the shapes:\n",
    "```python\n",
    "print(\"training data shape= {}; training label shape = {} \\ntest data shape= {}; test label shape = {}\".format(train_data.shape, train_label.shape,test_data.shape,test_label.shape))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into test and training and breaking dataset into data and label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and test sets:\n",
    "```python\n",
    "train_set = (train_data, train_label)\n",
    "test_set = (test_data, REPLACE_ME)\n",
    "```\n",
    "Replace REPLACE_ME with the relevant varaible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Training\n",
    "In this part we load the data from pre-processed files and train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion\n",
    "[Amazon Built-in Agorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) support csv and recordio/protobuf data formats. In this example we'll work with [recordio](https://github.com/eclesh/recordio).  \n",
    "\n",
    "```python\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype('float32')\n",
    "labels = np.array([t.tolist() for t in train_set[1]]).astype('float32')\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Conversion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload training data\n",
    "Now that we've created our recordIO-wrapped protobuf, we'll need to upload it to S3, so that Amazon SageMaker training can use it.\n",
    "\n",
    "Upload to S3:\n",
    "```python\n",
    "key = 'recordio-pb-data'\n",
    "boto3.resource('s3').Bucket(sagemaker_session.default_bucket()).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(sagemaker_session.default_bucket(), prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup an output S3 location for the model artifact that will be output as the result of training with the algorithm.\n",
    "```python\n",
    "output_location = 's3://{}/{}/output'.format(sagemaker_session.default_bucket(), prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output location\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model\n",
    "At this point we are using an linear learner from amazon algorithms. Docker file containing the model is located in multiple regions. We tool the following steps\n",
    "1. define containers\n",
    "2. Create am Estimator object and pass the hyper-parameters as well as the model location to it.\n",
    "3. run Estimator.fit to begin training the model\n",
    "\n",
    "SageMaker uses one of thse prebuilt containers for the linear-learner built in algo\n",
    "```python\n",
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/linear-learner:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/linear-learner:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:latest'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the containers dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Estimator function to create a new sagemaker trainning job\n",
    "```python\n",
    "sess = sagemaker.Session()\n",
    "linear = sagemaker.estimator.Estimator(containers[region],\n",
    "                                       role, #S3 role, so the notebook can read the data and upload the model\n",
    "                                       train_instance_count=1, #number of instances for training - leave at 1\n",
    "                                       train_instance_type='ml.m5.large', # type of training instance\n",
    "                                       output_path=output_location, #S3 location for the trained model\n",
    "                                       sagemaker_session=sess,\n",
    "                                       base_job_name='linear-learner-' + name)\n",
    "```\n",
    "Set the [hyperparamaeters](https://bit.ly/2D1qaLE). You'll need to use black magic and call up some data scientists friends to come up with the optimal values. [This might help](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSpVTGTOxh8D5xu0_ZYcvL4dHvGYVCP3piIcu6_j58xz_1kBnaLEA).\n",
    "Altenatively, we'll discuss Auto Model Tuning later today.\n",
    "```python\n",
    "l1 = REPLACE_ME # Choose value between 0.1 to 0.9\n",
    "mini_batch_size = REPLACE_ME # Choose a value between 100 and 5,000\n",
    "use_bias = REPLACE_ME # Choose True or False\n",
    "learning_rate = REPLACE_ME # choose a value between 0.001 to 0.9\n",
    "\n",
    "linear.set_hyperparameters(feature_dim=30, # dataset has 30 columns (features)\n",
    "                           predictor_type='binary_classifier',\n",
    "                           l1=l1,\n",
    "                           mini_batch_size=mini_batch_size,\n",
    "                           use_bias=use_bias,\n",
    "                           learning_rate=learning_rate)\n",
    "```\n",
    "send the link to S3 to the trainning job and start the trainning\n",
    "```python\n",
    "linear.fit({'train': s3_train_data})\n",
    "```\n",
    "\n",
    "This will start a Sagemaker job that will launch one or more instances to train the model. You can monitor the job in the [Sagemaker AWS console](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs). This should take ~2min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the Estimator function to create a new sagemaker trainning job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - Record how well your model did\n",
    "Tail to the end of the Traning output. To see how well training did search for the last occurrence of *train binary_classification_accuracy* value.  \n",
    "Share results with you teammates [here](https://docs.google.com/spreadsheets/d/1BNrvQHq1wAWjyBlN1t0v6KXmnVSzPIp73tmh_WMlHSc/edit#gid=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting the model\n",
    "We use sagemaker to host the live model by calling deploy from estimator we defined previously. This action will create a dockerized environment using ECS and permits autoscaling. \n",
    "\n",
    "```python\n",
    "linear_predictor = linear.deploy(initial_instance_count=1, #Initial number of instances. \n",
    "                                                           #Autoscaling can increase the number of instances.\n",
    "                                 instance_type='ml.t2.medium',# instance type\n",
    "                                 name='linear-learner-' + name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host a SageMaker endpoint for predicition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "deploy resturn a live endpoint (linear_predictor). Predictors in sagemaker accept csv and json. In this case we use json serialization.\n",
    "\n",
    "configure the predicition endpoint:\n",
    "```python\n",
    "linear_predictor.content_type = 'text/csv'\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict a single item:\n",
    "```python\n",
    "print(train_set[0][48:49])\n",
    "print(\"The data Actual label: \" + str(train_set[1][48:49][0]))\n",
    "linear_predictor.predict(train_set[0][48:49])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/) from the test data:\n",
    "```python\n",
    "non_zero = np.count_nonzero(test_set[1])\n",
    "zero = len(test_set[1]) - non_zero\n",
    "print(\"test set includes: {} non zero and {} items woth value zero\".format(non_zero, zero))\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(test_set[1], predictions, rownames=['actuals'], colnames=['predictions'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to explain what each of the 4 figures of the confusion matrix represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "if you're ready to be done with this notebook, please run the delete_endpoint line in the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
